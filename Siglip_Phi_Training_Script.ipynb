{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5-PkcrutwNs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import sys\n",
        "import subprocess\n",
        "import gc\n",
        "\n",
        "# try:\n",
        "#     subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"sentencepiece\"])\n",
        "#     logger.info(\"Successfully installed sentencepiece\")\n",
        "# except Exception as e:\n",
        "#     logger.error(f\"Failed to install sentencepiece: {e}\")\n",
        "#     sys.exit(1)\n",
        "\n",
        "# # Verify sentencepiece installation\n",
        "# try:\n",
        "#     import sentencepiece\n",
        "#     logger.info(f\"sentencepiece version: {sentencepiece.__version__}\")\n",
        "# except ImportError:\n",
        "#     logger.error(\"Failed to import sentencepiece after installation\")\n",
        "#     sys.exit(1)\n",
        "import sentencepiece\n",
        "# Now import transformers after sentencepiece is installed\n",
        "\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor, AutoModelForVision2Seq, AutoModel, SiglipModel, SiglipProcessor\n",
        "from transformers import get_scheduler\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "import random\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor, AutoModelForVision2Seq, AutoModel, SiglipModel, SiglipProcessor\n",
        "from transformers import get_scheduler\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "import random\n",
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"siglip_training.log\"),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Custom dataset for CIFAR10 image-answer pairs\n",
        "class CIFAR10QADataset(Dataset):\n",
        "    def __init__(self, csv_file, image_dir, tokenizer, processor, max_length=512):\n",
        "        \"\"\"\n",
        "        Initialize the dataset.\n",
        "\n",
        "        Args:\n",
        "            csv_file (str): Path to the CSV file containing image-answer pairs\n",
        "            image_dir (str): Directory containing the images\n",
        "            tokenizer: Tokenizer for text processing\n",
        "            processor: Processor for image processing\n",
        "            max_length (int): Maximum sequence length for text\n",
        "        \"\"\"\n",
        "        # Verify image directory exists\n",
        "        if not os.path.exists(image_dir):\n",
        "            raise ValueError(f\"Image directory {image_dir} does not exist\")\n",
        "\n",
        "        # Load and verify CSV file\n",
        "        try:\n",
        "            self.data = pd.read_csv(csv_file)\n",
        "            self.data['len'] = self.data['Answer'].apply(lambda x: len(x))\n",
        "            self.data = self.data[self.data['len']>10]\n",
        "            del self.data['len']\n",
        "\n",
        "\n",
        "            self.data.columns = ['image_path', 'question', 'answer']\n",
        "            logger.info(f\"CSV columns: {self.data.columns.tolist()}\")\n",
        "            required_columns = ['image_path', 'question', 'answer']\n",
        "            missing_columns = [col for col in required_columns if col not in self.data.columns]\n",
        "            if missing_columns:\n",
        "                raise ValueError(f\"CSV file is missing required columns: {missing_columns}\")\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Error loading CSV file {csv_file}: {str(e)}\")\n",
        "\n",
        "        self.image_dir = image_dir\n",
        "        self.tokenizer = tokenizer\n",
        "        self.processor = processor\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Log dataset statistics\n",
        "        logger.info(f\"Loaded {len(self.data)} image-answer pairs from {csv_file}\")\n",
        "        logger.info(f\"Image directory: {image_dir}\")\n",
        "\n",
        "        # Verify first few images can be loaded\n",
        "        self._verify_images()\n",
        "\n",
        "    def _verify_images(self, num_samples=5):\n",
        "        \"\"\"Verify that images can be loaded correctly.\"\"\"\n",
        "        sample_indices = np.random.choice(len(self.data), min(num_samples, len(self.data)), replace=False)\n",
        "        for idx in sample_indices:\n",
        "            try:\n",
        "                row = self.data.iloc[idx]\n",
        "                image_path = os.path.join(self.image_dir, row['image_path'])\n",
        "                logger.info(f\"Verifying image: {image_path}\")\n",
        "\n",
        "                if not os.path.exists(image_path):\n",
        "                    raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
        "\n",
        "                image = Image.open(image_path).convert('RGB')\n",
        "                # Try processing the image\n",
        "                processed = self.processor(images=image, return_tensors=\"pt\")\n",
        "                logger.info(f\"Successfully verified image: {image_path}\")\n",
        "                logger.info(f\"Processed image shape: {processed.pixel_values.shape}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error verifying image at index {idx}: {str(e)}\")\n",
        "                raise\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            # Get row data\n",
        "            row = self.data.iloc[idx]\n",
        "            logger.info(f\"Processing item {idx}: {row['image_path']}\")\n",
        "\n",
        "            # Construct image path\n",
        "            image_path = os.path.join(self.image_dir, row['image_path'])\n",
        "            if not os.path.exists(image_path):\n",
        "                raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
        "\n",
        "            # Load and process image\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            image_inputs = self.processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "            # Verify image inputs\n",
        "            if image_inputs.pixel_values is None:\n",
        "                raise ValueError(f\"Failed to process image: {image_path}\")\n",
        "\n",
        "            logger.info(f\"Processed image shape: {image_inputs.pixel_values.shape}\")\n",
        "\n",
        "            # Process text\n",
        "            text = f\"Question: {row['question']}\\nAnswer: {row['answer']}\"\n",
        "            text_inputs = self.tokenizer(\n",
        "                text,\n",
        "                max_length=self.max_length,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            # Ensure attention_mask exists\n",
        "            if 'attention_mask' not in text_inputs:\n",
        "                input_ids = text_inputs.input_ids.squeeze(0)\n",
        "                attention_mask = torch.ones_like(input_ids)\n",
        "                attention_mask[input_ids == self.tokenizer.pad_token_id] = 0\n",
        "            else:\n",
        "                attention_mask = text_inputs.attention_mask.squeeze(0)\n",
        "\n",
        "            # Create batch dictionary with correct shapes\n",
        "            batch = {\n",
        "                \"pixel_values\": image_inputs.pixel_values.squeeze(),  # Remove batch dimension from processor\n",
        "                \"input_ids\": text_inputs.input_ids.squeeze(0),\n",
        "                \"attention_mask\": attention_mask,\n",
        "                \"labels\": text_inputs.input_ids.squeeze(0).clone()\n",
        "            }\n",
        "\n",
        "            # Verify batch contents\n",
        "            for k, v in batch.items():\n",
        "                if v is None:\n",
        "                    raise ValueError(f\"Batch item {k} is None\")\n",
        "                logger.info(f\"Batch item {k} shape: {v.shape if hasattr(v, 'shape') else 'no shape'}\")\n",
        "\n",
        "            return batch\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading item at index {idx}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "# SigLiP model definition\n",
        "class SigLiPModel(nn.Module):\n",
        "    def __init__(self, vision_model, text_model, projection_dim=512):\n",
        "        super().__init__()\n",
        "        self.vision_model = vision_model\n",
        "        self.text_model = text_model\n",
        "\n",
        "        # Freeze the text model (Phi3)\n",
        "        for param in self.text_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Get the correct hidden size for vision model\n",
        "        # SigLiP models use 'hidden_size' or 'vision_hidden_size'\n",
        "        vision_hidden_size = getattr(vision_model.config, 'hidden_size',\n",
        "                                    getattr(vision_model.config, 'vision_hidden_size', 768))\n",
        "\n",
        "        # Vision encoder projection\n",
        "        self.vision_projection = nn.Sequential(\n",
        "            nn.Linear(vision_hidden_size, projection_dim),\n",
        "            nn.LayerNorm(projection_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(projection_dim, projection_dim)\n",
        "        )\n",
        "\n",
        "        # Get the correct hidden size for text model\n",
        "        # Phi3 models use 'hidden_size'\n",
        "        text_hidden_size = getattr(text_model.config, 'hidden_size', 2048)\n",
        "\n",
        "        # Text encoder projection\n",
        "        self.text_projection = nn.Sequential(\n",
        "            nn.Linear(text_hidden_size, projection_dim),\n",
        "            nn.LayerNorm(projection_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(projection_dim, projection_dim)\n",
        "        )\n",
        "\n",
        "        # Temperature parameter for contrastive loss\n",
        "        self.temperature = nn.Parameter(torch.ones([]) * 0.07)\n",
        "\n",
        "        # Cross-attention for fusion\n",
        "        self.cross_attention = nn.MultiheadAttention(\n",
        "            embed_dim=projection_dim,\n",
        "            num_heads=8,\n",
        "            dropout=0.1\n",
        "        )\n",
        "\n",
        "        # Get the correct vocab size for text model\n",
        "        vocab_size = getattr(text_model.config, 'vocab_size', 50257)\n",
        "\n",
        "        # Final projection for generation\n",
        "        self.final_projection = nn.Linear(projection_dim, vocab_size)\n",
        "\n",
        "    def forward(self, pixel_values, input_ids, attention_mask, labels=None):\n",
        "        batch_size = pixel_values.size(0)\n",
        "\n",
        "        # Ensure pixel_values has the correct shape [batch_size, channels, height, width]\n",
        "        if pixel_values.dim() == 3:\n",
        "            pixel_values = pixel_values.unsqueeze(0)\n",
        "\n",
        "        # Get vision features\n",
        "        try:\n",
        "            # Process each image in the batch individually to avoid memory issues\n",
        "            vision_features_list = []\n",
        "            for i in range(batch_size):\n",
        "                # Extract single image\n",
        "                single_image = pixel_values[i:i+1]  # Keep batch dimension\n",
        "\n",
        "                # Process with vision model\n",
        "                vision_outputs = self.vision_model(single_image)\n",
        "\n",
        "                # Extract features\n",
        "                if hasattr(vision_outputs, 'last_hidden_state'):\n",
        "                    features = vision_outputs.last_hidden_state[:, 0, :]  # Use [CLS] token\n",
        "                elif hasattr(vision_outputs, 'pooler_output'):\n",
        "                    features = vision_outputs.pooler_output\n",
        "                else:\n",
        "                    # Fallback to using the entire output\n",
        "                    features = vision_outputs[0][:, 0, :] if isinstance(vision_outputs, tuple) else vision_outputs[:, 0, :]\n",
        "\n",
        "                vision_features_list.append(features)\n",
        "\n",
        "            # Concatenate features\n",
        "            vision_features = torch.cat(vision_features_list, dim=0)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in vision model forward pass: {str(e)}\")\n",
        "            logger.error(f\"pixel_values shape: {pixel_values.shape}\")\n",
        "            raise\n",
        "\n",
        "        # Project vision features\n",
        "        vision_embeddings = self.vision_projection(vision_features)\n",
        "\n",
        "        # Get text features\n",
        "        text_outputs = self.text_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "\n",
        "        # Extract text features\n",
        "        if hasattr(text_outputs, 'hidden_states') and text_outputs.hidden_states:\n",
        "            text_features = text_outputs.hidden_states[-1][:, 0, :]  # Use [CLS] token\n",
        "        elif hasattr(text_outputs, 'last_hidden_state'):\n",
        "            text_features = text_outputs.last_hidden_state[:, 0, :]  # Use [CLS] token\n",
        "        else:\n",
        "            # Fallback to using the entire output\n",
        "            text_features = text_outputs[0][:, 0, :] if isinstance(text_outputs, tuple) else text_outputs[:, 0, :]\n",
        "\n",
        "        # Project text features\n",
        "        text_embeddings = self.text_projection(text_features)\n",
        "\n",
        "        # Cross-attention fusion\n",
        "        vision_embeddings = vision_embeddings.unsqueeze(1)  # Add sequence dimension\n",
        "        text_embeddings = text_embeddings.unsqueeze(1)  # Add sequence dimension\n",
        "\n",
        "        # Ensure shapes are correct for cross-attention\n",
        "        if vision_embeddings.size(0) != text_embeddings.size(0):\n",
        "            # Handle batch size mismatch\n",
        "            vision_embeddings = vision_embeddings.expand(text_embeddings.size(0), -1, -1)\n",
        "\n",
        "        fused_features, _ = self.cross_attention(\n",
        "            query=text_embeddings,\n",
        "            key=vision_embeddings,\n",
        "            value=vision_embeddings\n",
        "        )\n",
        "\n",
        "        # Project to vocabulary\n",
        "        logits = self.final_projection(fused_features.squeeze(1))\n",
        "\n",
        "        # Calculate loss if labels are provided\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            # Reshape logits and labels for loss calculation\n",
        "            logits = logits.view(-1, logits.size(-1))\n",
        "            labels = labels.view(-1)\n",
        "            loss = loss_fct(logits, labels)\n",
        "\n",
        "        # Ensure all outputs have the correct shape\n",
        "        vision_embeddings = vision_embeddings.squeeze(1)  # Remove sequence dimension\n",
        "        text_embeddings = text_embeddings.squeeze(1)  # Remove sequence dimension\n",
        "\n",
        "        return {\n",
        "            \"loss\": loss if loss is not None else torch.tensor(0.0, device=logits.device),\n",
        "            \"logits\": logits,\n",
        "            \"vision_embeddings\": vision_embeddings,\n",
        "            \"text_embeddings\": text_embeddings\n",
        "        }\n",
        "\n",
        "# Training function\n",
        "def train(args):\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    logger.info(f\"Using device: {device}\")\n",
        "\n",
        "    # Load models and tokenizers\n",
        "    logger.info(\"Loading models and tokenizers...\")\n",
        "\n",
        "    # Load vision model (SigLiP)\n",
        "    vision_processor = AutoProcessor.from_pretrained(args.vision_model_name)\n",
        "    vision_model = AutoModelForVision2Seq.from_pretrained(args.vision_model_name)\n",
        "\n",
        "    # Load text model (Phi3)\n",
        "    text_tokenizer = AutoTokenizer.from_pretrained(args.text_model_name)\n",
        "    text_model = AutoModelForCausalLM.from_pretrained(args.text_model_name)\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    logger.info(\"Creating dataset and dataloader...\")\n",
        "    dataset = CIFAR10QADataset(\n",
        "        csv_file=args.csv_file,\n",
        "        image_dir=args.image_dir,\n",
        "        tokenizer=text_tokenizer,\n",
        "        processor=vision_processor,\n",
        "        max_length=args.max_length\n",
        "    )\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=args.num_workers\n",
        "    )\n",
        "\n",
        "    # Create SigLiP model\n",
        "    logger.info(\"Creating SigLiP model...\")\n",
        "    model = SigLiPModel(\n",
        "        vision_model=vision_model,\n",
        "        text_model=text_model,\n",
        "        projection_dim=args.projection_dim\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    # Create optimizer\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=args.learning_rate,\n",
        "        weight_decay=args.weight_decay\n",
        "    )\n",
        "\n",
        "    # Create learning rate scheduler\n",
        "    num_training_steps = len(dataloader) * args.num_epochs\n",
        "    lr_scheduler = get_scheduler(\n",
        "        name=\"cosine\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=args.warmup_steps,\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    logger.info(\"Starting training...\")\n",
        "    model.train()\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(args.num_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{args.num_epochs}\")\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            # Move batch to device\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(\n",
        "                pixel_values=pixel_values,\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs[\"loss\"]\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip gradients\n",
        "            if args.max_grad_norm is not None:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Update progress bar\n",
        "            epoch_loss += loss.item()\n",
        "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        # Log epoch metrics\n",
        "        avg_epoch_loss = epoch_loss / len(dataloader)\n",
        "        logger.info(f\"Epoch {epoch+1}/{args.num_epochs}, Average Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (epoch + 1) % args.save_steps == 0:\n",
        "            checkpoint_dir = Path(args.output_dir) / f\"checkpoint-{epoch+1}\"\n",
        "            checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            model.save_pretrained(checkpoint_dir)\n",
        "            vision_processor.save_pretrained(checkpoint_dir)\n",
        "            text_tokenizer.save_pretrained(checkpoint_dir)\n",
        "\n",
        "            logger.info(f\"Saved checkpoint to {checkpoint_dir}\")\n",
        "\n",
        "    # Save final model\n",
        "    logger.info(f\"Saving final model to {args.output_dir}\")\n",
        "    model.save_pretrained(args.output_dir)\n",
        "    vision_processor.save_pretrained(args.output_dir)\n",
        "    text_tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(args):\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    logger.info(f\"Using device: {device}\")\n",
        "\n",
        "    # Load models and tokenizers\n",
        "    logger.info(\"Loading models and tokenizers...\")\n",
        "\n",
        "    # Load vision model (SigLiP)\n",
        "    vision_processor = AutoProcessor.from_pretrained(args.vision_model_name)\n",
        "    vision_model = AutoModelForVision2Seq.from_pretrained(args.vision_model_name)\n",
        "\n",
        "    # Load text model (Phi3)\n",
        "    text_tokenizer = AutoTokenizer.from_pretrained(args.text_model_name)\n",
        "    text_model = AutoModelForCausalLM.from_pretrained(args.text_model_name)\n",
        "\n",
        "    # Load trained SigLiP model\n",
        "    model = SigLiPModel(\n",
        "        vision_model=vision_model,\n",
        "        text_model=text_model,\n",
        "        projection_dim=args.projection_dim\n",
        "    )\n",
        "    model.load_state_dict(torch.load(os.path.join(args.output_dir, \"pytorch_model.bin\")))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    logger.info(\"Creating dataset and dataloader...\")\n",
        "    dataset = CIFAR10QADataset(\n",
        "        csv_file=args.eval_csv_file,\n",
        "        image_dir=args.image_dir,\n",
        "        tokenizer=text_tokenizer,\n",
        "        processor=vision_processor,\n",
        "        max_length=args.max_length\n",
        "    )\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=args.eval_batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=args.num_workers\n",
        "    )\n",
        "\n",
        "    # Evaluation loop\n",
        "    logger.info(\"Starting evaluation...\")\n",
        "    total_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            # Move batch to device\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(\n",
        "                pixel_values=pixel_values,\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs[\"loss\"]\n",
        "            logits = outputs[\"logits\"]\n",
        "\n",
        "            # Calculate metrics\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Get predictions\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            all_predictions.extend(predictions.cpu().numpy().tolist())\n",
        "            all_labels.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "    # Calculate average loss\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    logger.info(f\"Evaluation Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return avg_loss\n",
        "\n",
        "# Main function\n",
        "def main(csv_file, image_dir, eval_csv_file=None, output_dir=\"output\",\n",
        "         model_name=\"google/siglip-base-patch16-224\", batch_size=32,\n",
        "         num_epochs=10, learning_rate=5e-5, num_workers=4,\n",
        "         device=None, seed=42, save_every=1, gradient_accumulation_steps=4):\n",
        "    \"\"\"\n",
        "    Main function to train SigLiP model on CIFAR10 QA dataset.\n",
        "    \"\"\"\n",
        "    # Set device if not provided\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Set random seeds\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Create output directory\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Clear memory before loading models\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # Load tokenizer and processor\n",
        "    logger.info(\"Loading tokenizer and processor...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    processor = SiglipProcessor.from_pretrained(model_name)\n",
        "\n",
        "    # Create datasets\n",
        "    logger.info(\"Creating datasets...\")\n",
        "    train_dataset = CIFAR10QADataset(\n",
        "        csv_file,\n",
        "        image_dir,\n",
        "        tokenizer,\n",
        "        processor\n",
        "    )\n",
        "\n",
        "    eval_dataset = CIFAR10QADataset(\n",
        "        eval_csv_file,\n",
        "        image_dir,\n",
        "        tokenizer,\n",
        "        processor\n",
        "    ) if eval_csv_file else None\n",
        "\n",
        "    # Create data loaders with smaller batch size if needed\n",
        "    logger.info(\"Creating data loaders...\")\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=False  # Disable pin_memory to reduce memory usage\n",
        "    )\n",
        "\n",
        "    eval_loader = DataLoader(\n",
        "        eval_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=False  # Disable pin_memory to reduce memory usage\n",
        "    ) if eval_dataset else None\n",
        "\n",
        "    # Clear memory before loading models\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # Initialize model\n",
        "    logger.info(\"Loading vision model (SigLiP)...\")\n",
        "    # Load vision model (SigLiP) with reduced precision\n",
        "    vision_model = SiglipModel.from_pretrained(model_name, torch_dtype=torch.float16, trust_remote_code=True)\n",
        "\n",
        "    # Clear memory before loading text model\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    logger.info(\"Loading text model (Phi3)...\")\n",
        "    # Load text model (Phi3) with reduced precision\n",
        "    text_model_name = \"microsoft/phi-2\"\n",
        "    text_tokenizer = AutoTokenizer.from_pretrained(text_model_name)\n",
        "    text_model = AutoModelForCausalLM.from_pretrained(text_model_name, torch_dtype=torch.float16)\n",
        "\n",
        "    # Clear memory before creating combined model\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    logger.info(\"Creating SigLiP model...\")\n",
        "    # Create SigLiP model with both vision and text models\n",
        "    model = SigLiPModel(\n",
        "        vision_model=vision_model,\n",
        "        text_model=text_model,\n",
        "        projection_dim=512\n",
        "    )\n",
        "\n",
        "    # Enable gradient checkpointing to save memory\n",
        "    model.vision_model.gradient_checkpointing_enable()\n",
        "    model.text_model.gradient_checkpointing_enable()\n",
        "\n",
        "    # Move model to device in a memory-efficient way\n",
        "    if torch.cuda.is_available():\n",
        "        # Move model to device in chunks with error handling\n",
        "        try:\n",
        "            # First move the vision model\n",
        "            model.vision_model = model.vision_model.to(device)\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            # Then move the text model\n",
        "            model.text_model = model.text_model.to(device)\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            # Finally move the remaining components\n",
        "            for name, param in model.named_parameters():\n",
        "                if not name.startswith(('vision_model.', 'text_model.')):\n",
        "                    param.data = param.data.to(device)\n",
        "                    if param.grad is not None:\n",
        "                        param.grad.data = param.grad.data.to(device)\n",
        "                    torch.cuda.empty_cache()\n",
        "                    gc.collect()\n",
        "        except RuntimeError as e:\n",
        "            logger.error(f\"Error moving model to device: {str(e)}\")\n",
        "            logger.error(\"Trying to free up memory and retry...\")\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            # Try again with a smaller batch size\n",
        "            batch_size = batch_size // 2\n",
        "            logger.info(f\"Reduced batch size to {batch_size}\")\n",
        "            return main(csv_file, image_dir, eval_csv_file, output_dir,\n",
        "                       model_name, batch_size, num_epochs, learning_rate,\n",
        "                       num_workers, device, seed, save_every,\n",
        "                       gradient_accumulation_steps)\n",
        "    else:\n",
        "        model = model.to(device)\n",
        "\n",
        "    # Initialize optimizer and scheduler\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    num_training_steps = len(train_loader) * num_epochs // gradient_accumulation_steps\n",
        "    lr_scheduler = get_scheduler(\n",
        "        \"linear\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    best_eval_loss = float('inf')\n",
        "    for epoch in range(num_epochs):\n",
        "        # Train\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        optimizer.zero_grad()  # Zero gradients at the start of epoch\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        for i, batch in enumerate(progress_bar):\n",
        "            try:\n",
        "                # Move batch to device in a memory-efficient way\n",
        "                for k, v in batch.items():\n",
        "                    if v is None:\n",
        "                        raise ValueError(f\"Batch item {k} is None\")\n",
        "                    batch[k] = v.to(device)\n",
        "                    logger.info(f\"Batch item {k} shape: {v.shape if hasattr(v, 'shape') else 'no shape'}\")\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(**batch)\n",
        "                if outputs[\"loss\"] is None:\n",
        "                    raise ValueError(\"Model returned None loss\")\n",
        "                loss = outputs[\"loss\"] / gradient_accumulation_steps  # Scale loss for gradient accumulation\n",
        "                loss.backward()\n",
        "\n",
        "                # Update weights after accumulating gradients\n",
        "                if (i + 1) % gradient_accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
        "                    optimizer.step()\n",
        "                    lr_scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                train_loss += loss.item() * gradient_accumulation_steps\n",
        "                progress_bar.set_postfix({\"loss\": loss.item() * gradient_accumulation_steps})\n",
        "\n",
        "                # Clear cache more frequently to free up memory\n",
        "                if i % 5 == 0 and torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "                    gc.collect()\n",
        "\n",
        "                # Delete intermediate tensors to free memory\n",
        "                del outputs\n",
        "                del loss\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error processing batch {i}: {str(e)}\")\n",
        "                logger.error(f\"Batch contents: {[(k, v.shape if hasattr(v, 'shape') else 'no shape') for k, v in batch.items()]}\")\n",
        "                raise\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        logger.info(f\"Epoch {epoch + 1} - Average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Clear memory after training\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        # Evaluate\n",
        "        if eval_loader:\n",
        "            model.eval()\n",
        "            eval_loss = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in eval_loader:\n",
        "                    try:\n",
        "                        # Move batch to device in a memory-efficient way\n",
        "                        for k, v in batch.items():\n",
        "                            if v is None:\n",
        "                                raise ValueError(f\"Batch item {k} is None\")\n",
        "                            batch[k] = v.to(device)\n",
        "\n",
        "                        outputs = model(**batch)\n",
        "                        if outputs[\"loss\"] is None:\n",
        "                            raise ValueError(\"Model returned None loss\")\n",
        "                        eval_loss += outputs[\"loss\"].item()\n",
        "\n",
        "                        # Delete intermediate tensors to free memory\n",
        "                        del outputs\n",
        "\n",
        "                        # Clear cache more frequently to free up memory\n",
        "                        if torch.cuda.is_available():\n",
        "                            torch.cuda.empty_cache()\n",
        "                            gc.collect()\n",
        "\n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"Error processing evaluation batch: {str(e)}\")\n",
        "                        logger.error(f\"Batch contents: {[(k, v.shape if hasattr(v, 'shape') else 'no shape') for k, v in batch.items()]}\")\n",
        "                        raise\n",
        "\n",
        "            avg_eval_loss = eval_loss / len(eval_loader)\n",
        "            logger.info(f\"Epoch {epoch + 1} - Average evaluation loss: {avg_eval_loss:.4f}\")\n",
        "\n",
        "            # Save best model\n",
        "            if avg_eval_loss < best_eval_loss:\n",
        "                best_eval_loss = avg_eval_loss\n",
        "                model.save_pretrained(output_dir / \"best_model\")\n",
        "                tokenizer.save_pretrained(output_dir / \"best_model\")\n",
        "                logger.info(f\"Saved best model with evaluation loss: {best_eval_loss:.4f}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (epoch + 1) % save_every == 0:\n",
        "            checkpoint_dir = output_dir / f\"checkpoint-{epoch + 1}\"\n",
        "            model.save_pretrained(checkpoint_dir)\n",
        "            tokenizer.save_pretrained(checkpoint_dir)\n",
        "            logger.info(f\"Saved checkpoint at epoch {epoch + 1}\")\n",
        "\n",
        "        # Clear memory after epoch\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "    logger.info(\"Training completed!\")\n",
        "\n",
        "\n",
        "\n",
        "# For command line usage\n",
        "main(\n",
        "    csv_file=\"cifar10_qa_results.csv\",\n",
        "    image_dir=\"cifar10_images\",\n",
        "    eval_csv_file=\"cifar10_qa_results.csv\",\n",
        "    output_dir=\"cifaroutput\",\n",
        "    batch_size=32,\n",
        "    num_epochs=10,\n",
        "    model_name = \"google/siglip-base-patch16-224\"\n",
        ")"
      ]
    }
  ]
}